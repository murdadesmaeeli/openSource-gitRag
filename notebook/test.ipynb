{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "# Get the current working directory and navigate one level up\n",
    "PROJECT_ROOT = Path.cwd().parent  # Go one level above the current working directory\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import logging\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize OpenAI async client\n",
    "client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'),base_url=os.getenv('BASE_URL'))\n",
    "\n",
    "# Token Rate Limit Configuration\n",
    "# token_limit_per_minute = 2000000\n",
    "token_limit_per_minute = 2000000\n",
    "char_limit=200000\n",
    "token_counter = []\n",
    "token_counter_time_window = 60  # seconds\n",
    "\n",
    "# Chunk text by characters\n",
    "def chunk_text_by_characters(text, char_limit=char_limit):\n",
    "    for i in range(0, len(text), char_limit):\n",
    "        yield text[i:i + char_limit]\n",
    "\n",
    "# Rate limiting enforcement\n",
    "async def enforce_rate_limit():\n",
    "    current_time = time.time()\n",
    "    while token_counter and token_counter[0] < current_time - token_counter_time_window:\n",
    "        token_counter.pop(0)\n",
    "\n",
    "    if len(token_counter) >= token_limit_per_minute:\n",
    "        earliest_token_time = token_counter[0]\n",
    "        sleep_time = (earliest_token_time + token_counter_time_window) - current_time\n",
    "        if sleep_time > 0:\n",
    "            logging.info(f\"Rate limit approaching. Sleeping for {sleep_time:.2f} seconds.\")\n",
    "            await asyncio.sleep(sleep_time)\n",
    "\n",
    "# GPT-4 API Call\n",
    "async def call_gpt4(question, text_chunk, max_retries=5):\n",
    "    prompt = f\"\"\"\n",
    "    You are going to look at the file contents in separate chunks based on the chunks that are returned. \n",
    "    Return a JSON with key \\\"response\\\" containing a list of file paths starting with 'srcRepo'.\n",
    "\n",
    "    Goal/Question: {question}\n",
    "\n",
    "    Context:\n",
    "    {text_chunk}\n",
    "    \"\"\"\n",
    "\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        await enforce_rate_limit()\n",
    "        try:\n",
    "            response = await client.chat.completions.create(\n",
    "                # model=\"gpt-4o\",\n",
    "                model=\"us.amazon.nova-micro-v1:0\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                top_p=0.95,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "\n",
    "            usage = response.usage\n",
    "            total_tokens_used = usage.total_tokens\n",
    "            token_counter.extend([time.time()] * total_tokens_used)\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "\n",
    "        retries += 1\n",
    "        sleep_time = 2 ** retries\n",
    "        logging.warning(f\"Retrying after {sleep_time} seconds...\")\n",
    "        await asyncio.sleep(sleep_time)\n",
    "\n",
    "    logging.error(\"Max retries exceeded.\")\n",
    "    return None\n",
    "\n",
    "# Process chunks asynchronously with concurrency control\n",
    "async def process_text_chunks(question, text_chunks, max_concurrent_requests=(token_limit_per_minute//char_limit)-1):\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "\n",
    "    async def sem_task(chunk):\n",
    "        async with semaphore:\n",
    "            return await call_gpt4(question, chunk)\n",
    "\n",
    "    tasks = [sem_task(chunk) for chunk in text_chunks]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# Main execution\n",
    "async def main(question):\n",
    "    input_file = 'tmp/file_tree.txt'\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        text_chunks = list(chunk_text_by_characters(text, char_limit=128000))\n",
    "        results = await process_text_chunks(question, text_chunks, max_concurrent_requests=5)\n",
    "\n",
    "        with open('results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        logging.info(\"Processing complete. Results saved to 'results.json'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Input file '{input_file}' not found.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "\n",
    "# Apply nest_asyncio for environments like Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Example usage\n",
    "user_question = \"how to get current month cloud costs with boto3 sdk?\"\n",
    "asyncio.run(main(user_question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import mimetypes\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# # Navigate to project root\n",
    "# PROJECT_ROOT = Path(__file__).resolve().parent.parent\n",
    "# os.chdir(PROJECT_ROOT)\n",
    "\n",
    "def is_text_file(file_path):\n",
    "    text_extensions = {\n",
    "        '.txt', '.md', '.py', '.js', '.java', '.c', '.cpp', '.h', '.css',\n",
    "        '.html', '.xml', '.json', '.yaml', '.yml', '.ini', '.conf', '.sh',\n",
    "        '.bat', '.csv', '.log'\n",
    "    }\n",
    "\n",
    "    if os.path.splitext(file_path)[1].lower() in text_extensions:\n",
    "        return True\n",
    "\n",
    "    mime_type, _ = mimetypes.guess_type(file_path)\n",
    "    if mime_type and mime_type.startswith('text'):\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            chunk = f.read(1024)\n",
    "            return not bool(b'\\x00' in chunk)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def summarize_folder(folder_path, summary_file):\n",
    "    subfolders = []\n",
    "    files = []\n",
    "\n",
    "    for item in sorted(os.listdir(folder_path)):\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            subfolders.append(item)\n",
    "        elif os.path.isfile(item_path):\n",
    "            files.append(item)\n",
    "\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Summary of '{os.path.basename(folder_path)}':\\n\\n\")\n",
    "        if subfolders:\n",
    "            f.write(\"Subfolders:\\n\")\n",
    "            for sub in subfolders:\n",
    "                f.write(f\"- {sub}\\n\")\n",
    "        if files:\n",
    "            f.write(\"\\nFiles:\\n\")\n",
    "            for file in files:\n",
    "                f.write(f\"- {file}\\n\")\n",
    "\n",
    "def create_repo_with_summaries(source_directory, target_directory):\n",
    "    if not os.path.exists(target_directory):\n",
    "        os.makedirs(target_directory)\n",
    "\n",
    "    for root, dirs, files in os.walk(source_directory):\n",
    "        relative_path = os.path.relpath(root, source_directory)\n",
    "        target_root = os.path.join(target_directory, relative_path)\n",
    "\n",
    "        if not os.path.exists(target_root):\n",
    "            os.makedirs(target_root)\n",
    "\n",
    "        for file in files:\n",
    "            source_file = os.path.join(root, file)\n",
    "            target_file = os.path.join(target_root, file)\n",
    "            if is_text_file(source_file):\n",
    "                with open(source_file, 'r', encoding='utf-8', errors='ignore') as src, \\\n",
    "                     open(target_file, 'w', encoding='utf-8') as tgt:\n",
    "                    tgt.write(src.read())\n",
    "\n",
    "        for dir_name in dirs:\n",
    "            summary_file = os.path.join(target_root, f\"{dir_name}-gitRag.txt\")\n",
    "            summarize_folder(os.path.join(root, dir_name), summary_file)\n",
    "\n",
    "# Example Usage\n",
    "create_repo_with_summaries('gitRagRepo', 'newGitRagRepo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import logging\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize OpenAI async client\n",
    "client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url=os.getenv('BASE_URL'))\n",
    "\n",
    "# Token Rate Limit Configuration\n",
    "token_limit_per_minute = 2000000\n",
    "char_limit = 200000\n",
    "token_counter = []\n",
    "token_counter_time_window = 60  # seconds\n",
    "\n",
    "# Rate limiting enforcement\n",
    "async def enforce_rate_limit():\n",
    "    current_time = time.time()\n",
    "    while token_counter and token_counter[0] < current_time - token_counter_time_window:\n",
    "        token_counter.pop(0)\n",
    "\n",
    "    if len(token_counter) >= token_limit_per_minute:\n",
    "        earliest_token_time = token_counter[0]\n",
    "        sleep_time = (earliest_token_time + token_counter_time_window) - current_time\n",
    "        if sleep_time > 0:\n",
    "            logging.info(f\"Rate limit approaching. Sleeping for {sleep_time:.2f} seconds.\")\n",
    "            await asyncio.sleep(sleep_time)\n",
    "\n",
    "# GPT-4 API Call\n",
    "async def should_traverse(question, summary_text, max_retries=5):\n",
    "    prompt = f\"\"\"\n",
    "    Based on the provided summary, should we traverse deeper into this subfolder for the given question?\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Summary:\n",
    "    {summary_text}\n",
    "\n",
    "    Respond with a JSON object: {{\"traverse\": true or false}}\n",
    "    \"\"\"\n",
    "\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        await enforce_rate_limit()\n",
    "        try:\n",
    "            response = await client.chat.completions.create(\n",
    "                model=\"us.amazon.nova-micro-v1:0\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                top_p=0.95,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "\n",
    "            usage = response.usage\n",
    "            total_tokens_used = usage.total_tokens\n",
    "            token_counter.extend([time.time()] * total_tokens_used)\n",
    "            return json.loads(response.choices[0].message.content)['traverse']\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "\n",
    "        retries += 1\n",
    "        sleep_time = 2 ** retries\n",
    "        logging.warning(f\"Retrying after {sleep_time} seconds...\")\n",
    "        await asyncio.sleep(sleep_time)\n",
    "\n",
    "    logging.error(\"Max retries exceeded.\")\n",
    "    return False\n",
    "\n",
    "async def traverse_repo(question, directory):\n",
    "    files_of_interest = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        summary_files = [f for f in files if f.endswith('-gitrag.txt')]\n",
    "\n",
    "        traverse_decisions = await asyncio.gather(*[\n",
    "            should_traverse(question, open(os.path.join(root, sf), 'r', encoding='utf-8').read())\n",
    "            for sf in summary_files\n",
    "        ])\n",
    "\n",
    "        for idx, decision in enumerate(traverse_decisions):\n",
    "            if not decision:\n",
    "                dirs.remove(summary_files[idx].replace('-gitrag.txt', ''))\n",
    "\n",
    "        for file in files:\n",
    "            if not file.endswith('-gitrag.txt'):\n",
    "                files_of_interest.append(os.path.join(root, file))\n",
    "\n",
    "    return files_of_interest\n",
    "\n",
    "async def main(question):\n",
    "    repo_path = 'newGitRagRepo'\n",
    "    files_to_consider = await traverse_repo(question, repo_path)\n",
    "\n",
    "    with open('results.json', 'w') as f:\n",
    "        json.dump(files_to_consider, f, indent=2)\n",
    "\n",
    "    logging.info(f\"Traversal complete. Files of interest saved to 'results.json'.\")\n",
    "\n",
    "# Apply nest_asyncio for environments like Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Example usage\n",
    "user_question = \"how to get current month cloud costs with boto3 sdk?\"\n",
    "asyncio.run(main(user_question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
