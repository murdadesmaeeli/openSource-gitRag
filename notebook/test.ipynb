{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "# Get the current working directory and navigate one level up\n",
    "PROJECT_ROOT = Path.cwd().parent  # Go one level above the current working directory\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import logging\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize OpenAI async client\n",
    "client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Token Rate Limit Configuration\n",
    "token_limit_per_minute = 2000000\n",
    "token_counter = []\n",
    "token_counter_time_window = 60  # seconds\n",
    "\n",
    "# Chunk text by characters\n",
    "def chunk_text_by_characters(text, char_limit=128000):\n",
    "    for i in range(0, len(text), char_limit):\n",
    "        yield text[i:i + char_limit]\n",
    "\n",
    "# Rate limiting enforcement\n",
    "async def enforce_rate_limit():\n",
    "    current_time = time.time()\n",
    "    while token_counter and token_counter[0] < current_time - token_counter_time_window:\n",
    "        token_counter.pop(0)\n",
    "\n",
    "    if len(token_counter) >= token_limit_per_minute:\n",
    "        earliest_token_time = token_counter[0]\n",
    "        sleep_time = (earliest_token_time + token_counter_time_window) - current_time\n",
    "        if sleep_time > 0:\n",
    "            logging.info(f\"Rate limit approaching. Sleeping for {sleep_time:.2f} seconds.\")\n",
    "            await asyncio.sleep(sleep_time)\n",
    "\n",
    "# GPT-4 API Call\n",
    "async def call_gpt4(question, text_chunk, max_retries=5):\n",
    "    prompt = f\"\"\"\n",
    "    You are going to look at the file contents in separate chunks based on the chunks that are returned. \n",
    "    Return a JSON with key \\\"response\\\" containing a list of file paths starting with 'srcRepo'.\n",
    "\n",
    "    Goal/Question: {question}\n",
    "\n",
    "    Context:\n",
    "    {text_chunk}\n",
    "    \"\"\"\n",
    "\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        await enforce_rate_limit()\n",
    "        try:\n",
    "            response = await client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                top_p=0.95,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "\n",
    "            usage = response.usage\n",
    "            total_tokens_used = usage.total_tokens\n",
    "            token_counter.extend([time.time()] * total_tokens_used)\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "\n",
    "        retries += 1\n",
    "        sleep_time = 2 ** retries\n",
    "        logging.warning(f\"Retrying after {sleep_time} seconds...\")\n",
    "        await asyncio.sleep(sleep_time)\n",
    "\n",
    "    logging.error(\"Max retries exceeded.\")\n",
    "    return None\n",
    "\n",
    "# Process chunks asynchronously with concurrency control\n",
    "async def process_text_chunks(question, text_chunks, max_concurrent_requests=5):\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "\n",
    "    async def sem_task(chunk):\n",
    "        async with semaphore:\n",
    "            return await call_gpt4(question, chunk)\n",
    "\n",
    "    tasks = [sem_task(chunk) for chunk in text_chunks]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# Main execution\n",
    "async def main(question):\n",
    "    input_file = 'cleaned_file_tree.txt'\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        text_chunks = list(chunk_text_by_characters(text, char_limit=128000))\n",
    "        results = await process_text_chunks(question, text_chunks, max_concurrent_requests=5)\n",
    "\n",
    "        with open('results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        logging.info(\"Processing complete. Results saved to 'results.json'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Input file '{input_file}' not found.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "\n",
    "# Apply nest_asyncio for environments like Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Example usage\n",
    "user_question = \"Which files need updates based on the new security guidelines?\"\n",
    "asyncio.run(main(user_question))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
